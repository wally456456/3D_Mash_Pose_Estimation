# 논문명 : 3DPW: Recovering Accurate 3D Human Pose in The Wild Using IMUs and a Moving Camera(ECCV 2018)
### 저자명 : Timo von Marcard, Roberto Henschel, Michael J. Black, Bodo Rosenhahn, Gerard  Pons-Moll

# Abstract
- Single hand-held camera와 몸의 팔,다리에 Inertial Measurement Unit(IMU)를 활용하여 wild한 환경에서의 3D poses를 추측하는 방법론을 소개한다
- 3D pose detected된 것을 새로운 graph based optimization을 통해서 2D로 바꿔준다(3차원 현실 -> 2차원 이미지)

# Introduction
- 이 논문은 두가지의 연관된 목표가 있다
	- 야외의 3D human pose를 multiple people, interacting with the environment한 상황에서 수행할 수 있게 하는 것( 사람의 팔,다리에 부착된 IMU와 손으로 든 phone camera 의 video 데이터를 합해서 사용한다)


- IMU-based system은 사람이 착용을 하는 것이기 때문에 공간의 제약을 받지 않음
	- 하지만 정확도가 몇가지의 요인들 때문에 저하된다
		- 1. 초기의 pose가 잘못 측정되면 bone misalignment를 발생시킨다
		- 2. Head drift : 일정 시간이 지나면 각 IMU 는 다른 coordinate frame을 측정한다(영점이 달라진다)
		- 3. Positional drift때문에 global position이 정확하게 측정되지 않는다
			- positional drift : 위치적으로 서서히 포류하는 것

- 따라서 저자들은 Video Inertial Poser(VIP)라는 6~17개의 IMU를 부착하거나 hand-held moving phone camera를 통해 joint를 이용해서 pose를 추측하는 방법론을 제시한다
- IMU의 문제점 다시한번
	- 첫번째로 사진이나 동영상에 사람이 감지되어야 하고 IMU data와 연관이 되어있어야 한다
	- 두번째로 IMU는 heading drift때문에 부정확하다
	- 세번째 추측된 3D pose들은 moving camera에 맞게 align되어야 한다
	- 그 외의 문제점들 : 완벽한 가림 multiple people, 카메라 view에서 나가는 사람
- 위의 문제점들을 해결하기 위해서 새로운 graph-based association 방법론, 연속적인 pose optimization scheme(sequence안에 모든 프레임들을 고려하는)를 제시한다, noise와 incomplete 데이터를 처리하기 위해서 SMPL을 활용해 준다

- 저자들의 방법론은 3가지 단계를 거친다
	- 1. Initialization : 초기 3D pose를 SMPL안에 넣어서 IMU의 orientation과 맞게 만들어준다
	- 2. Association : 3D pose와 2D person detection을 single binary quadratic optimization 을 사용하여 연관시킨다
	- 3. Data fusion : 목적함수를 정의하여 주고 full sequence, per-sensor heading errors, camera pose and translation 의 3D poses를 joint단계에서 optimize시킨다 
	- 모델의 목적함수는 `model의 orientation과 acceleration이 IMU의 값`과` SMPL에서 추출한 3D joints와 2D CNN detection`이 가까울 때 최소화 된다

# Background
- SMPL Body Model
	- 저자들은 인간의 shape parameter를 SMPL model에 fitting하기 위해 optimize한다
	- Shape가 고정이 되면 저자들의 목적은 pose parameter ![plot](https://user-images.githubusercontent.com/69032315/148216822-3264860d-f2d0-4ac5-bad6-963157d33939.png)
 를 6개의 global translation과 rotation, 23개의 joints의 상대적인 회전 representation으로 복구하는 것이다 
		- 저자들은 standard forward kinematics를 활용해서 ![plot](https://user-images.githubusercontent.com/69032315/148216831-2b96f609-a47e-40e0-a338-352fa92333ab.png)
 를 rigid transformation ![plot](https://user-images.githubusercontent.com/69032315/148216845-a0ae2e4e-676d-4556-bbe7-f4dd608f8cee.png)
로 변환시킨다(B = Bone)
	- Local bone coordinate frame ![plot](https://user-images.githubusercontent.com/69032315/148216881-324f45f2-6d7c-41ac-a63d-5d86b146d4b5.png)
 를 global SMPL frame ![plot](https://user-images.githubusercontent.com/69032315/148216885-4991f4ba-d69c-46d2-b13e-4affa73651cd.png)
 로 매핑하기 위해서![plot](https://user-images.githubusercontent.com/69032315/148216902-2f5d7d73-f8cf-4de0-b52a-82a5b4fae6ca.png)
 = bone transformation 은 `rotation`과 `translation` 두개의 요소로 구성된다
	- Coordinate Frames : 궁극적으로 저자들은 포즈 파라메터 ![plot](https://user-images.githubusercontent.com/69032315/148216912-d0f9fff0-0f17-497a-8799-29e79f0dea9e.png)
 가 IMU의 신호와 같은 bone orientation을 만들게 하는 것이 목적이다
		- IMU들은 global coordinate frame ![plot](https://user-images.githubusercontent.com/69032315/148216918-d4c06c58-02f5-4f16-83b1-2a085570d72a.png)
 의 상대적인 local coordinate frame  ![plot](https://user-images.githubusercontent.com/69032315/148216931-d5da6d25-4a68-4e5b-bb14-a5b4e48469a1.png)
(of sensor box)의 orientation을 측정한다 하지만 ![plot](https://user-images.githubusercontent.com/69032315/148216937-dcc430b8-5bd1-4927-b46b-3105ea33394b.png)
 의  ![plot](https://user-images.githubusercontent.com/69032315/148216945-1c4cebed-27e8-4bee-afb3-180ade3a47d1.png)
(coordinate frame of SMPL)과 다른 형식을 취한다 -> 그래서 다음과 같은 식으로 같게 만들어주는 transformation을 취한다 ![plot](https://user-images.githubusercontent.com/69032315/148216971-ec59868f-33fa-43b1-bab4-c397c4904aa4.png)
(Transformation은 constant한 값을 가진다고 가정을 한다)
		- 또한 센서에서 얻은 값 ![plot](https://user-images.githubusercontent.com/69032315/148217006-349116a7-0c2f-4292-a81f-66462bcdfae2.png)
 (Transformation from sensor to SMPL Bone)를 SMPL에 맞게 변환을 해줘야한다->  ![plot](https://user-images.githubusercontent.com/69032315/148217010-5aa16f23-d68c-4e42-b40d-cce25bced161.png)
(SMPL bone orientation)은 첫 프레임에서  를 활용하여 얻을 수 있다 
	- 영상의 첫 프레임에서  과  (IMU 신호)를 가지고 다음과 같은 식을 얻을 수 있었다
 ![plot](https://user-images.githubusercontent.com/69032315/148217019-5dfc2e76-b0a9-4894-9894-aa8e9db6aaf4.png)
  -  ![plot](https://user-images.githubusercontent.com/69032315/148217039-453621cb-5940-4011-976c-9cda1fbc7929.png)
를 활용하여  ![plot](https://user-images.githubusercontent.com/69032315/148217049-d5764bd5-611b-4e07-828a-620297a1a5e4.png)
를 SMPL frame에 매핑되게 한다
	- 저자들은 센서들의 부착위치가 움직임에 따라 변하지 않는다고 가정을 한다 따라서,  ![plot](https://user-images.githubusercontent.com/69032315/148217056-5d285225-3951-4279-b7b5-12e82ae2ea90.png)
를 초기 pose 파라메터 ![plot](https://user-images.githubusercontent.com/69032315/148217071-d4db0dfd-9cae-4287-828a-76865301e770.png)
 와 IMU orientation의 첫 프레임을 가지고 계산한다

![plot](https://user-images.githubusercontent.com/69032315/148217080-182e9d07-4cb9-4c85-a341-ccd3cb8df752.png)

- Heading Drift : IMU의 orientation 측정값들은 자기적 disturbance때문에 값이 불분명 해진다 -> 시간이 지날수록 ![plot](https://user-images.githubusercontent.com/69032315/148217107-c5c4b2c3-6dd5-4712-aa96-895b5a5fe221.png)
 의 값이 실제값과 달라지는 것
- 이 Drift ![plot](https://user-images.githubusercontent.com/69032315/148217123-5e5199aa-ac34-45f0-aa58-04b022f0f3b2.png)
 는 original global inertial frame  ![plot](https://user-images.githubusercontent.com/69032315/148217131-4749c337-5f9c-4c72-8fc8-f5af77b2be66.png)
를 disturbed inertial frame ![plot](https://user-images.githubusercontent.com/69032315/148217138-ec93574a-a73b-4e34-97c6-3f9a2930765b.png)
 로 만든다 –> 더 최악인 것은 이 drift는 기기마다 다 다른 방식으로 표현된다 -> 모델의 파라메터로 넣어주어서 optimization의 일부로 만들어준다 
	- one parameter rotation ![plot](https://user-images.githubusercontent.com/69032315/148217153-19d382ee-7cf1-4359-ad94-500b62b5eb64.png)
 , ![plot](https://user-images.githubusercontent.com/69032315/148217161-d57919fd-fe23-469c-97cb-40991c14eca9.png)
 = rotation angle -> 하나의 IMU sensor로부터 받은 anlges는 ![plot](https://user-images.githubusercontent.com/69032315/148217175-708d310a-0b5a-4cf1-bf07-f4bde1a8e6c6.png)
 로 정의된다 , heading error는 천천히 변화하기 때문에 저자들은 하나의 고정된 파라메터로만 각 sequence를 처리한다

# Video Inertial Poser(VIP)
- 손으로 들고 찍은 video와 IMU에서 정확한 3D human motion capture를 하기 위해서 다음 3가지의 단계를 거친다 -> 1. Initialization, 2.Pose candidate Association, 3.Video-inertial fushion

- Initialization
	- SMPL 모델의 bone orientation에 측정된 IMU orientation을 투입시켜 Initial 3D pose를 얻어낸다
	- Meausred bone orientation ![plot](https://user-images.githubusercontent.com/69032315/148217200-3d62bbf5-b1b1-47b2-a68e-ca047f67132f.png)
 는 다음과 같이 얻을 수 있다
	
  - ![plot](https://user-images.githubusercontent.com/69032315/148217204-807a97d6-ce2a-47ee-bae9-02381b6b11c7.png)

	-  ![plot](https://user-images.githubusercontent.com/69032315/148217272-05e3f1ba-51ff-4bd2-96f7-333d25defaba.png)
는 constant bone to sensor offset Eq1을 의미, ![plot](https://user-images.githubusercontent.com/69032315/148217280-6f2e8c91-0a13-4710-bf6b-7360418647dd.png)
 의 합은 sensor to global frame의 rotational map을 의미한다
	- 실제 bone orientation값과 측정된 bone orientation값의 Rotational 불일치를 다음과 같이 측정한다
- ![plot](https://user-images.githubusercontent.com/69032315/148217293-87f4e417-acb7-45a1-ab0e-921d69a4ccdf.png)

	- ![plot](https://user-images.githubusercontent.com/69032315/148217326-c344265b-4993-4e15-8a09-1f55bf17d54a.png) -operator = 해당 axis-angle parameter

	- IMU의 discrepancies의 합이 가장 낮을 때의 특정 시점 t의 3D initial pose를 다음과 같이 찾는다
 ![plot](https://user-images.githubusercontent.com/69032315/148217337-c5a70abf-e266-47e0-8a7f-34cf7bd9de85.png)
	- ![plot](https://user-images.githubusercontent.com/69032315/148217343-332771d1-274d-481a-8e2a-41239a0a714a.png)= pose prior wighted by ![plot](https://user-images.githubusercontent.com/69032315/148217364-87509eaf-bfa4-41d5-9a5d-e2edea292657.png)
 ,  ![plot](https://user-images.githubusercontent.com/69032315/148217371-ef606e9e-f911-4bcb-b783-8be65a8825a0.png)
를 가우시안분포를 따르게 하고 joint limit의 범위 안에 제한 시키기 위한 것
	- 첫 iteration에서는 heading angle![plot](https://user-images.githubusercontent.com/69032315/148217388-c1bbfbe3-b301-4ef4-b626-47f6ef69139d.png)
 의 값을 알 수가 없다 -> 신체의 상대적인 IMU의 위치를 가지고 알아낸다 -> Eq(1)에서 ![plot](https://user-images.githubusercontent.com/69032315/148217411-e5668c93-4f09-419c-88b1-dc3ec3d679f6.png)
 를 구하기위해 필요한 bone offset ![plot](https://user-images.githubusercontent.com/69032315/148217418-b3d36077-a36e-4677-b10c-e08b566a2d1a.png)
 을 rough하게 알 수 있다 

- Pose Candidate Assignment
	- CNN방법론을 사용해서 2D pose detection v를 얻고  ![plot](https://user-images.githubusercontent.com/69032315/148217435-a903e3bf-646f-4ab7-87c2-f04098279b11.png)
개의 관절에 confidence score를 부여해준다
	- 각 2D pose v 를 3D pose candidate로 바꾸기 위해서 저자들은 undirected weighted graph ![plot](https://user-images.githubusercontent.com/69032315/148217450-f79819bb-5900-49cd-bc28-0ca228a2d608.png)
를 사용한다
	- V = v를 모아놓은 것 
	-  ![plot](https://user-images.githubusercontent.com/69032315/148217481-f88ac027-0158-4db5-9008-63729f88b137.png)(가설)
		- 3D pose  ![plot](https://user-images.githubusercontent.com/69032315/148217495-9ffae776-bd27-43bd-b86b-915247ddc392.png)
of  person l in the frame t, 
		- 가설이 선택이 된다면  ![plot](https://user-images.githubusercontent.com/69032315/148217505-66f31431-cc1a-4e96-9559-2a3a025bfc64.png)
이 1이되고 채택이 되지 못한다면 0이 된다
		- 가설을 사용한 이유는 각 hypothesis에 cost를 열거해주고 total cost가 가장 적게 하는 hypothesis를 채택하는 것이다
		- 가장 total cost가 적게 나오는 가설을 찾는 것은 다음과 같은 목적함수를 최소화하는 것으로 수행한다
  - ![plot](https://user-images.githubusercontent.com/69032315/148217522-82b84fb8-b164-4e49-85e0-f6d50dc32f0b.png)

 
	- Edge sets![plot](https://user-images.githubusercontent.com/69032315/148217542-9951826d-0bc7-4d5f-8d08-80b01de3fa2f.png)
 는 모든 pairs of 2D poses 를 포함하고 있다 -> 이것은 가설을 고르는데 사용된다 
	- Eq(6)의 (a)는 2D pose v가 최대 1개의 사람한테 배정되게 하는 것 
	- Eq(6)의 (b)는 각 person이 최대 한 개의 2D pose detection v in frame t 에 배정되게 하는 것이다
	- Eq(5)는 unary(일진법의)  가 2D to 3D consistency를 측정하게 하는 것이고  는 다른 hypothesis의 값과의 pair-wise cost를 측정해주는 것

- Unary Costs
	- 가설 H = H(l,v)의 2D to 3D consistency를 측정하기 위해서 hypothesis camera ![plot](https://user-images.githubusercontent.com/69032315/148223586-54293b4a-bb25-4f77-a687-366e0a4e1d88.png)
 를 3D landmark of  ![plot](https://user-images.githubusercontent.com/69032315/148223599-acdab9d7-c9ea-44dc-a995-89ef546bac43.png)
와 2D detected v와의 re-projection error를 최소화해서 얻어낸다
	- per landmark reprojection error ![plot](https://user-images.githubusercontent.com/69032315/148223642-582757fc-42b9-415e-909b-5381f9252610.png)
 는 confidence score ![plot](https://user-images.githubusercontent.com/69032315/148223694-428f1630-d41d-49d8-8709-1875ac6f0ee3.png)
 를 이용해서 weighted되서 구해진다
	- 총 Consistency ![plot](https://user-images.githubusercontent.com/69032315/148223702-915e359b-2255-43de-a70e-98d7ca844454.png)
는 모든 weighted residuals ![plot](https://user-images.githubusercontent.com/69032315/148223711-b47fea5f-b712-4321-9fef-1b8b006c457c.png)
 를 평균을 내는 방식으로 측정된다 -> 카메라의 거리에 따라 이 측정값들이 심하게 바뀐다 -> 이것을 조절해주기 위해서  ![plot](https://user-images.githubusercontent.com/69032315/148223719-99c7483f-dfac-4142-8eb7-7f96eff5fa20.png)
(카메라 센터)와 average 3D joint의 거리를 scaling해주고 feature를 얻는다 

![plot](https://user-images.githubusercontent.com/69032315/148223724-a2ec3e11-1929-4789-a712-3d35eb192875.png)


- Pairwise Costs
	- 두개의 hypothesis ![plot](https://user-images.githubusercontent.com/69032315/148223761-4d13ad8f-d786-4973-b0fa-9e62e18fd67e.png)
의 consistency를 측정하기 위해서 두가지의 edge connect hypothesis를 설정한다
		- 1. Inter-frame : 두개의 Hypothesis 가 같은 사람에 대한 반응을 보이고 있다가 30보다 적은 프레임동안 분리된다고 할 경우 -> Camera hypothesis ![plot](https://user-images.githubusercontent.com/69032315/148223813-583894a2-43f8-48ec-b96b-9fe4a415f641.png)
 의  respective root joint position ![plot](https://user-images.githubusercontent.com/69032315/148223826-adca61fe-008d-44a0-b8b3-1f2badd4e402.png)
과 Orientation ![plot](https://user-images.githubusercontent.com/69032315/148223833-96e7582b-b008-4249-87f3-f59e13fbfd23.png)
는 많은 차이를 보이면 안된다, variation은 두 시간 간의 일시적 차이인 |t – t’|이다.
	- ![plot](https://user-images.githubusercontent.com/69032315/148223845-9851d0f5-7d53-4da7-bfed-9d08385f8416.png)
	-  ![plot](https://user-images.githubusercontent.com/69032315/148223882-2cd909c9-900f-4962-b10b-80a753d6ef3b.png)
는 root joint translation과 orientation consistency를 측정하고  은 두 프레임의 시간적 distance를 의미한다 
	-   ![plot](https://user-images.githubusercontent.com/69032315/148223902-3b4e9e88-6ff1-45aa-81c1-60a0f0565f9e.png)
= rotational part of  ![plot](https://user-images.githubusercontent.com/69032315/148223922-5b7f7139-2ed4-488a-a3aa-c99f84464f0d.png)
, ![plot](https://user-images.githubusercontent.com/69032315/148223939-9b69e5c0-330b-4c2b-9f67-79e5cedcee1c.png)
  =  ![plot](https://user-images.githubusercontent.com/69032315/148223950-d8c99ae5-f8c1-443b-af80-caad36f1d0ee.png)
과  ![plot](https://user-images.githubusercontent.com/69032315/148223955-87e3f69e-6690-4bb7-ae83-eb23f528e6e2.png)
의 거리를 구해준다(Eq3과 같이)
		- Intra-frame : 두개의 H,H’ 가설들이 같은 프레임에서 다른 사람을 고려할 때 이다, camera hypothesis center는 같다, 두개의 일치성을 측정하기 위해서 meta-camera hypothesis  ![plot](https://user-images.githubusercontent.com/69032315/148224022-e946d95b-2a9b-4557-908a-f4d8f36fd78c.png)
를 두 가설의 reprojection error를 최소화 시켜서 측정해준다

![plot](https://user-images.githubusercontent.com/69032315/148224035-6037a199-8d17-444d-aba1-bb88705afb86.png)
	- 위의 식은 특정 카메라의 ![plot](https://user-images.githubusercontent.com/69032315/148224087-186c3a21-998e-43cd-8694-06b9822dcf9c.png)
중심 가 meta camera  ![plot](https://user-images.githubusercontent.com/69032315/148224091-5b7243a9-664c-449a-a4a2-275f1714ed5a.png)
의 center의 차이를 측정한다

- Graph Optimization
	- Eq(5)의 Graph labeling 문제는 NP-hard(시간내에 풀 수 없는 문제)이기 때문에 저자들은 linearized formulation Eq(5)을 사용해서 해결하였다
	- 각 ![plot](https://user-images.githubusercontent.com/69032315/148224100-9d13dbbf-7c6c-49e6-9c1d-a4dba31d573b.png)
 를 binary auxiliary variable  ![plot](https://user-images.githubusercontent.com/69032315/148224117-d0027b6d-a94a-4a83-af42-9dc12a7eb9ca.png)
로 대체한 다음에 constraints를 더해주었다

- Video-Inertial Data Fusion
	- Assignment 문제가 해결되면 2D pose를 jointly 최적화 된 pose, camera pose, heading angles를 다음과 같은 식을 최소화하면서 얻을 수 있다

![plot](https://user-images.githubusercontent.com/69032315/148224137-53de4769-24ae-423d-8653-81fbd8d67ed4.png)
	- Θ = 각 actor, frame마다의 pose parameter,  ![plot](https://user-images.githubusercontent.com/69032315/148224160-a33fdf0d-912f-40cc-9e90-2aae7f751d21.png)
= IMU heading correction angles의 벡터, ![plot](https://user-images.githubusercontent.com/69032315/148224165-2fa2d361-3d45-48a1-8998-ec5ec785ffb4.png)
 = camera poses for each frame, ![plot](https://user-images.githubusercontent.com/69032315/148224187-4d764a7b-0b88-4436-a4c3-4ac316810607.png)
 는 `IMU orientation`, `IMU acceleration`, `image information`을 의미한다 각 term은 가중치 w로 weighted된다

- Orientation Term : Eq(4)를 모든 frame  를 고려한다는 의미이다

![plot](https://user-images.githubusercontent.com/69032315/148224207-4efa4c40-1f45-49a1-9ba7-46757bb7072e.png)
	- ![plot](https://user-images.githubusercontent.com/69032315/148224261-f2e8c5ed-37ac-4ee2-850f-e3e5fb8a90d1.png)
 (camera coordinate system)에서 ![plot](https://user-images.githubusercontent.com/69032315/148224269-c63f6473-a819-463d-9e30-4c678d5dc9ce.png)
 로의 rotation 매핑을 camera pose M의 rotational 부분을 inverse해서 얻을 수 있기 때문에 위의 term 은 camera IMU를 포함한다고 할 수 있다

- Acceleration Term
	- 측정된 IMU acceleration과 해당 model의 vertex의 acceleration의 일치를 enforce 한다
	- IMU acceleration in world coordinate은 다음과 같이 표현된다 (s = sensor, t = time)
 ![plot](https://user-images.githubusercontent.com/69032315/148224280-2a70a76b-016e-4d0e-a94c-8fc10141db4c.png)
	-   ![plot](https://user-images.githubusercontent.com/69032315/148224289-cdd2c9ce-9547-4306-a70f-692166ea9f85.png)
= gravity in global coordinate,  ![plot](https://user-images.githubusercontent.com/69032315/148224320-c28be688-f5a2-41d3-967a-c81f39c7ae06.png)
(해당 SMPL vertex acceleration)은 근사된다
	- Acceleration Term은  ![plot](https://user-images.githubusercontent.com/69032315/148224359-00404cf2-f2a9-43c2-a7dd-875787d6bc87.png)
, ![plot](https://user-images.githubusercontent.com/69032315/148224370-207a3d6b-8994-4437-8def-1e4bbc696013.png)
의 acceleration의 측정값과 예측값 을 quadratic norm of deviation을 수행해 준다

 ![plot](https://user-images.githubusercontent.com/69032315/148224381-8692cb25-e928-4ecf-a144-515004205462.png)
	- Eq(15)는 camera IMU의 측정된 acceleration을 포함하고 있고 해당 global coordinate의 camera center의 acceleration도 포함되어 있다

- Image Term
	- 모든  ![plot](https://user-images.githubusercontent.com/69032315/148224408-22d30ade-d927-4f95-80dc-332ab2bf1e00.png)
의 랜드마크와 모든 프레임  ![plot](https://user-images.githubusercontent.com/69032315/148224419-1211795d-e8e0-4563-b433-f512dd01d0da.png)
의 re-projection 오차를 축적한다

![plot](https://user-images.githubusercontent.com/69032315/148224427-05aea373-21f0-4443-944d-f5563fc7b5a8.png)
	- ![plot](https://user-images.githubusercontent.com/69032315/148224435-deafa952-a843-46c1-bc8f-3b882fb0b04d.png)
 = confidence score with a landmark
 
- Prior Term
	- Eq(4)의 형식과 같다
	- 모든 pose Θ를 축적 시켰고 ![plot](https://user-images.githubusercontent.com/69032315/148224457-a62a692d-e862-4fdb-9ad2-40bc515a2649.png) (number of pose)로 스케일링 시켜주었다












