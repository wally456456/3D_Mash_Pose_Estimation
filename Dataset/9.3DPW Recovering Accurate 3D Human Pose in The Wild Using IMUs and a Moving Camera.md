# 논문명 : 3DPW: Recovering Accurate 3D Human Pose in The Wild Using IMUs and a Moving Camera(ECCV 2018)
### 저자명 : Timo von Marcard, Roberto Henschel, Michael J. Black, Bodo Rosenhahn, Gerard  Pons-Moll

# Abstract
- Single hand-held camera와 몸의 팔,다리에 Inertial Measurement Unit(IMU)를 활용하여 wild한 환경에서의 3D poses를 추측하는 방법론을 소개한다
- 3D pose detected된 것을 새로운 graph based optimization을 통해서 2D로 바꿔준다(3차원 현실 -> 2차원 이미지)

# Introduction
- 이 논문은 두가지의 연관된 목표가 있다
	- 1. 야외의 3D human pose를 multiple people, interacting with the environment한 상황에서 수행할 수 있게 하는 것( 사람의 팔,다리에 부착된 IMU와 손으로 든 phone camera 의 video 데이터를 합해서 사용한다)
	- 2. 

- IMU-based system은 사람이 착용을 하는 것이기 때문에 공간의 제약을 받지 않음
	- 하지만 정확도가 몇가지의 요인들 때문에 저하된다
		- 1. 초기의 pose가 잘못 측정되면 bone misalignment를 발생시킨다
		- 2. Head drift : 일정 시간이 지나면 각 IMU 는 다른 coordinate frame을 측정한다(영점이 달라진다)
		- 3. Positional drift때문에 global position이 정확하게 측정되지 않는다
			- positional drift : 위치적으로 서서히 포류하는 것

- 따라서 저자들은 Video Inertial Poser(VIP)라는 6~17개의 IMU를 부착하거나 hand-held moving phone camera를 통해 joint를 이용해서 pose를 추측하는 방법론을 제시한다
- IMU의 문제점 다시한번
	- 첫번째로 사진이나 동영상에 사람이 감지되어야 하고 IMU data와 연관이 되어있어야 한다
	- 두번째로 IMU는 heading drift때문에 부정확하다
	- 세번째 추측된 3D pose들은 moving camera에 맞게 align되어야 한다
	- 그 외의 문제점들 : 완벽한 가림 multiple people, 카메라 view에서 나가는 사람
- 위의 문제점들을 해결하기 위해서 새로운 graph-based association 방법론, 연속적인 pose optimization scheme(sequence안에 모든 프레임들을 고려하는)를 제시한다, noise와 incomplete 데이터를 처리하기 위해서 SMPL을 활용해 준다

- 저자들의 방법론은 3가지 단계를 거친다
	- 1. Initialization : 초기 3D pose를 SMPL안에 넣어서 IMU의 orientation과 맞게 만들어준다
	- 2. Association : 3D pose와 2D person detection을 single binary quadratic optimization 을 사용하여 연관시킨다
	- 3. Data fusion : 목적함수를 정의하여 주고 full sequence, per-sensor heading errors, camera pose and translation 의 3D poses를 joint단계에서 optimize시킨다 
	- 모델의 목적함수는 `model의 orientation과 acceleration이 IMU의 값`과` SMPL에서 추출한 3D joints와 2D CNN detection`이 가까울 때 최소화 된다

# Background
- SMPL Body Model
	- 저자들은 인간의 shape parameter를 SMPL model에 fitting하기 위해 optimize한다
	- Shape가 고정이 되면 저자들의 목적은 pose parameter ![image](https://user-images.githubusercontent.com/69032315/148216822-3264860d-f2d0-4ac5-bad6-963157d33939.png)
 를 6개의 global translation과 rotation, 23개의 joints의 상대적인 회전 representation으로 복구하는 것이다 
		- 저자들은 standard forward kinematics를 활용해서 ![image](https://user-images.githubusercontent.com/69032315/148216831-2b96f609-a47e-40e0-a338-352fa92333ab.png)
 를 rigid transformation ![image](https://user-images.githubusercontent.com/69032315/148216845-a0ae2e4e-676d-4556-bbe7-f4dd608f8cee.png)
로 변환시킨다(B = Bone)
	- Local bone coordinate frame ![image](https://user-images.githubusercontent.com/69032315/148216881-324f45f2-6d7c-41ac-a63d-5d86b146d4b5.png)
 를 global SMPL frame ![image](https://user-images.githubusercontent.com/69032315/148216885-4991f4ba-d69c-46d2-b13e-4affa73651cd.png)
 로 매핑하기 위해서![image](https://user-images.githubusercontent.com/69032315/148216902-2f5d7d73-f8cf-4de0-b52a-82a5b4fae6ca.png)
 = bone transformation 은 `rotation`과 `translation` 두개의 요소로 구성된다
	- Coordinate Frames : 궁극적으로 저자들은 포즈 파라메터 ![image](https://user-images.githubusercontent.com/69032315/148216912-d0f9fff0-0f17-497a-8799-29e79f0dea9e.png)
 가 IMU의 신호와 같은 bone orientation을 만들게 하는 것이 목적이다
		- IMU들은 global coordinate frame ![image](https://user-images.githubusercontent.com/69032315/148216918-d4c06c58-02f5-4f16-83b1-2a085570d72a.png)
 의 상대적인 local coordinate frame  ![image](https://user-images.githubusercontent.com/69032315/148216931-d5da6d25-4a68-4e5b-bb14-a5b4e48469a1.png)
(of sensor box)의 orientation을 측정한다 하지만 ![image](https://user-images.githubusercontent.com/69032315/148216937-dcc430b8-5bd1-4927-b46b-3105ea33394b.png)
 의  ![image](https://user-images.githubusercontent.com/69032315/148216945-1c4cebed-27e8-4bee-afb3-180ade3a47d1.png)
(coordinate frame of SMPL)과 다른 형식을 취한다 -> 그래서 다음과 같은 식으로 같게 만들어주는 transformation을 취한다 ![image](https://user-images.githubusercontent.com/69032315/148216971-ec59868f-33fa-43b1-bab4-c397c4904aa4.png)
(Transformation은 constant한 값을 가진다고 가정을 한다)
		- 또한 센서에서 얻은 값 ![image](https://user-images.githubusercontent.com/69032315/148217006-349116a7-0c2f-4292-a81f-66462bcdfae2.png)
 (Transformation from sensor to SMPL Bone)를 SMPL에 맞게 변환을 해줘야한다->  ![image](https://user-images.githubusercontent.com/69032315/148217010-5aa16f23-d68c-4e42-b40d-cce25bced161.png)
(SMPL bone orientation)은 첫 프레임에서  를 활용하여 얻을 수 있다 
	- 영상의 첫 프레임에서  과  (IMU 신호)를 가지고 다음과 같은 식을 얻을 수 있었다
 ![image](https://user-images.githubusercontent.com/69032315/148217019-5dfc2e76-b0a9-4894-9894-aa8e9db6aaf4.png)
  -  ![image](https://user-images.githubusercontent.com/69032315/148217039-453621cb-5940-4011-976c-9cda1fbc7929.png)
를 활용하여  ![image](https://user-images.githubusercontent.com/69032315/148217049-d5764bd5-611b-4e07-828a-620297a1a5e4.png)
를 SMPL frame에 매핑되게 한다
	- 저자들은 센서들의 부착위치가 움직임에 따라 변하지 않는다고 가정을 한다 따라서,  ![image](https://user-images.githubusercontent.com/69032315/148217056-5d285225-3951-4279-b7b5-12e82ae2ea90.png)
를 초기 pose 파라메터 ![image](https://user-images.githubusercontent.com/69032315/148217071-d4db0dfd-9cae-4287-828a-76865301e770.png)
 와 IMU orientation의 첫 프레임을 가지고 계산한다

![image](https://user-images.githubusercontent.com/69032315/148217080-182e9d07-4cb9-4c85-a341-ccd3cb8df752.png)

- Heading Drift : IMU의 orientation 측정값들은 자기적 disturbance때문에 값이 불분명 해진다 -> 시간이 지날수록 ![image](https://user-images.githubusercontent.com/69032315/148217107-c5c4b2c3-6dd5-4712-aa96-895b5a5fe221.png)
 의 값이 실제값과 달라지는 것
- 이 Drift ![image](https://user-images.githubusercontent.com/69032315/148217123-5e5199aa-ac34-45f0-aa58-04b022f0f3b2.png)
 는 original global inertial frame  ![image](https://user-images.githubusercontent.com/69032315/148217131-4749c337-5f9c-4c72-8fc8-f5af77b2be66.png)
를 disturbed inertial frame ![image](https://user-images.githubusercontent.com/69032315/148217138-ec93574a-a73b-4e34-97c6-3f9a2930765b.png)
 로 만든다 –> 더 최악인 것은 이 drift는 기기마다 다 다른 방식으로 표현된다 -> 모델의 파라메터로 넣어주어서 optimization의 일부로 만들어준다 
	- one parameter rotation ![image](https://user-images.githubusercontent.com/69032315/148217153-19d382ee-7cf1-4359-ad94-500b62b5eb64.png)
 , ![image](https://user-images.githubusercontent.com/69032315/148217161-d57919fd-fe23-469c-97cb-40991c14eca9.png)
 = rotation angle -> 하나의 IMU sensor로부터 받은 anlges는 ![image](https://user-images.githubusercontent.com/69032315/148217175-708d310a-0b5a-4cf1-bf07-f4bde1a8e6c6.png)
 로 정의된다 , heading error는 천천히 변화하기 때문에 저자들은 하나의 고정된 파라메터로만 각 sequence를 처리한다

# Video Inertial Poser(VIP)
- 손으로 들고 찍은 video와 IMU에서 정확한 3D human motion capture를 하기 위해서 다음 3가지의 단계를 거친다 -> 1. Initialization, 2.Pose candidate Association, 3.Video-inertial fushion

- Initialization
	- SMPL 모델의 bone orientation에 측정된 IMU orientation을 투입시켜 Initial 3D pose를 얻어낸다
	- Meausred bone orientation ![image](https://user-images.githubusercontent.com/69032315/148217200-3d62bbf5-b1b1-47b2-a68e-ca047f67132f.png)
 는 다음과 같이 얻을 수 있다
	
  - ![image](https://user-images.githubusercontent.com/69032315/148217204-807a97d6-ce2a-47ee-bae9-02381b6b11c7.png)

	-  ![image](https://user-images.githubusercontent.com/69032315/148217272-05e3f1ba-51ff-4bd2-96f7-333d25defaba.png)
는 constant bone to sensor offset Eq1을 의미, ![image](https://user-images.githubusercontent.com/69032315/148217280-6f2e8c91-0a13-4710-bf6b-7360418647dd.png)
 의 합은 sensor to global frame의 rotational map을 의미한다
	- 실제 bone orientation값과 측정된 bone orientation값의 Rotational 불일치를 다음과 같이 측정한다
- ![image](https://user-images.githubusercontent.com/69032315/148217293-87f4e417-acb7-45a1-ab0e-921d69a4ccdf.png)

	- ![image](https://user-images.githubusercontent.com/69032315/148217326-c344265b-4993-4e15-8a09-1f55bf17d54a.png) -operator = 해당 axis-angle parameter

	- IMU의 discrepancies의 합이 가장 낮을 때의 특정 시점 t의 3D initial pose를 다음과 같이 찾는다
 ![image](https://user-images.githubusercontent.com/69032315/148217337-c5a70abf-e266-47e0-8a7f-34cf7bd9de85.png)
	- ![image](https://user-images.githubusercontent.com/69032315/148217343-332771d1-274d-481a-8e2a-41239a0a714a.png)= pose prior wighted by ![image](https://user-images.githubusercontent.com/69032315/148217364-87509eaf-bfa4-41d5-9a5d-e2edea292657.png)
 ,  ![image](https://user-images.githubusercontent.com/69032315/148217371-ef606e9e-f911-4bcb-b783-8be65a8825a0.png)
를 가우시안분포를 따르게 하고 joint limit의 범위 안에 제한 시키기 위한 것
	- 첫 iteration에서는 heading angle![image](https://user-images.githubusercontent.com/69032315/148217388-c1bbfbe3-b301-4ef4-b626-47f6ef69139d.png)
 의 값을 알 수가 없다 -> 신체의 상대적인 IMU의 위치를 가지고 알아낸다 -> Eq(1)에서 ![image](https://user-images.githubusercontent.com/69032315/148217411-e5668c93-4f09-419c-88b1-dc3ec3d679f6.png)
 를 구하기위해 필요한 bone offset ![image](https://user-images.githubusercontent.com/69032315/148217418-b3d36077-a36e-4677-b10c-e08b566a2d1a.png)
 을 rough하게 알 수 있다 

- Pose Candidate Assignment
	- CNN방법론을 사용해서 2D pose detection v를 얻고  ![image](https://user-images.githubusercontent.com/69032315/148217435-a903e3bf-646f-4ab7-87c2-f04098279b11.png)
개의 관절에 confidence score를 부여해준다
	- 각 2D pose v 를 3D pose candidate로 바꾸기 위해서 저자들은 undirected weighted graph ![image](https://user-images.githubusercontent.com/69032315/148217450-f79819bb-5900-49cd-bc28-0ca228a2d608.png)
를 사용한다
	- V = v를 모아놓은 것 
	-  ![image](https://user-images.githubusercontent.com/69032315/148217481-f88ac027-0158-4db5-9008-63729f88b137.png)(가설)
		- 3D pose  ![image](https://user-images.githubusercontent.com/69032315/148217495-9ffae776-bd27-43bd-b86b-915247ddc392.png)
of  person l in the frame t, 
		- 가설이 선택이 된다면  ![image](https://user-images.githubusercontent.com/69032315/148217505-66f31431-cc1a-4e96-9559-2a3a025bfc64.png)
이 1이되고 채택이 되지 못한다면 0이 된다
		- 가설을 사용한 이유는 각 hypothesis에 cost를 열거해주고 total cost가 가장 적게 하는 hypothesis를 채택하는 것이다
		- 가장 total cost가 적게 나오는 가설을 찾는 것은 다음과 같은 목적함수를 최소화하는 것으로 수행한다
  - ![image](https://user-images.githubusercontent.com/69032315/148217522-82b84fb8-b164-4e49-85e0-f6d50dc32f0b.png)

 
	- Edge sets![image](https://user-images.githubusercontent.com/69032315/148217542-9951826d-0bc7-4d5f-8d08-80b01de3fa2f.png)
 는 모든 pairs of 2D poses 를 포함하고 있다 -> 이것은 가설을 고르는데 사용된다 
	- Eq(6)의 (a)는 2D pose v가 최대 1개의 사람한테 배정되게 하는 것 
	- Eq(6)의 (b)는 각 person이 최대 한 개의 2D pose detection v in frame t 에 배정되게 하는 것이다
	- Eq(5)는 unary(일진법의)  가 2D to 3D consistency를 측정하게 하는 것이고  는 다른 hypothesis의 값과의 pair-wise cost를 측정해주는 것

- Unary Costs
	- 가설 H = H(l,v)의 2D to 3D consistency를 측정하기 위해서 hypothesis camera  를 3D landmark of  와 2D detected v와의 re-projection error를 최소화해서 얻어낸다










