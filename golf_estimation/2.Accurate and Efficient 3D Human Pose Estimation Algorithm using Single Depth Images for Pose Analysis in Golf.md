# 논문명 : Accurate  and Efﬁcient 3D Human Pose  Estimation Algorithm using  Single  Depth  Images for Pose  Analysis  in Golf(CVPR 2017)
### 저자명 : Soonchan  Park,  Ju Yong Chang,  Hyuk Jeong,  Jae-Ho  Lee,  Ji-Young  Park


모르는 단어 : penetration(침투)


# Introduction
- 기존의 방법론들은 특별한 장치를 입어야 되기 때문에 편의성이 떨어진다
- 저자들의 방법론은 2017년 당시에 나왔던 모델들보다 정확성, 비용적 측면에서 나은 성과를 냄
- 딥러닝을 사용하지 않고 RandomForest + Voting의 방식으로 진행한다


# Ground truth data
- I =  ![plot](https://user-images.githubusercontent.com/69032315/149350915-9c3a708e-5dde-4f82-b230-8e25f2ea9ded.png) = distance from the depth camera to the scene for each pixel p
- J = ![plot](https://user-images.githubusercontent.com/69032315/149354362-a717eba6-7a34-427a-8997-ebb60dcf4f55.png) , where  ,![plot](https://user-images.githubusercontent.com/69032315/149354397-7d877fe6-5199-4c93-bf18-141d7def014a.png) = ground truth joints

- Data acquisition
	- KinectV2, Vicon을 사용해서 모션을 캡처하고 I,J를 얻어냈다
	- Kinect = 30FPS를 뽑아내는 time-of-flight depth camera
	- Vicon = 17개의 적외선 카메라, marker를 가림 현상 없이 추출하기 위한 장비
	- Vicon의 Plug-in-Gait Marker Placement를 이용해서 120 FPS에 21개의 joints를 추출(head, neck, shoulder L/R, elbow L/R, wrist L/R, hand L/R, hip L/R, knee L/R, ankle L/R, foot L/R, shoulder spine, middle spine, and base spine)
	- 두 센서의 프레임을 맞춰 주기 위해서 OpenCV의 tool을 이용해준다 
	- 다양한 유형의 swing을 데이터를 얻기 위해서 대상에게 8개의 유형의 스윙을 주문하였다(Sway, Reverse spine angle, Hanging back, Chicken wing, Early extension, Wide stance, and Narrow stance, Free swing)
	- 카메라에서의 거리 또한 1.5~3m로 다양하게 촬영을 하여 14명의 사람으로 총 10,476개의 프레임의 golf swing data를 구하였다

- Data set arrangement
	- background를 없애 주는 작업을 해 줌 -> background에 large constant number 부여
	- Shoulder spine, middle spine, and base spine 부위의 ground truth를 제거해주어서 J = 18개의 joints로 구성된다
	- Penetration(침투 여기서는 가림 현상)을 각 관절에 대한 ![plot](https://user-images.githubusercontent.com/69032315/149354471-d0ca28eb-61b5-4afe-ac93-785eb721b3f8.png) 로 maximum penetration depth를 정의해주었다 

  - ![plot](https://user-images.githubusercontent.com/69032315/149354489-e176f4d6-a905-4470-a808-f8b32fb60e26.png)

	- V = visibility of joint j
 
	-  ![plot](https://user-images.githubusercontent.com/69032315/149354575-adf8abda-6f0e-406e-92cd-673857b355a6.png) = 2D pixel position, ![plot](https://user-images.githubusercontent.com/69032315/149354592-52066865-5b77-46b3-a992-ea9a0c76fee0.png)  = joint
	- 최종 데이터셋의 형식은 다음과 같이 표현된다 S = (I, J, V)


# Human pose estimation algorithm
  - 2개의 유형의 decision tree를 사용한다
		- 1. ![plot](https://user-images.githubusercontent.com/69032315/149354688-8d796cbe-a382-4c29-8605-37f63e743fb9.png)  = foreground pixels를 ground truth joints로 학습시키게 하는 것, ![plot](https://user-images.githubusercontent.com/69032315/149354734-bf10bbf7-b6fd-44b4-9a6f-d76b06772f0d.png) 의 학습데이터는 ![plot](https://user-images.githubusercontent.com/69032315/149354750-74505fc4-4017-4c97-babb-92d5c51c4e32.png) , Pr = set of sampled pixels, ![plot](https://user-images.githubusercontent.com/69032315/149354784-570d4b56-23db-463d-ae55-fbb010e00775.png)  = set of 3D vectors from sampled pixels to ground truth joints

- Types of decision trees
		- 2. verification decision tree  ![plot](https://user-images.githubusercontent.com/69032315/149354794-4b7eabe4-2be1-4889-8121-7c4e7a50dedf.png) =  ![plot](https://user-images.githubusercontent.com/69032315/149354811-e9811b5f-af10-4a8a-b1ab-357a59b6487e.png)에서 얻은 값을 voting해주는 것(Foreground pixels의 votes 들이 투영된 위치를 분석함으로 해서), 
		- ![plot](https://user-images.githubusercontent.com/69032315/149354840-cc19cb07-1822-45e4-8a7f-2813075a1652.png)  = 보이는 관절에 대한 investigation -> pixel이 어느 관절에 대한 값인지에 대한 investigation, 학습과정에서 ground truth와 사전에 정의된 threshold  의 거리 안에 있는 pixel 들을 샘플링 해준다( 의 값은 경험적으로 6으로 정했다)
		-  ![plot](https://user-images.githubusercontent.com/69032315/149354862-48488132-a52c-47af-9638-7dae0864a7ce.png)
		-  ![plot](https://user-images.githubusercontent.com/69032315/149354873-db2880ed-92ab-4e7c-bc78-e7955025def9.png) = 보이는 관절에 대한 sampled pixels, ![plot](https://user-images.githubusercontent.com/69032315/149354890-bd128e74-8c0f-45e9-af45-b24ecc719eb0.png)  = V가 1인 ground truth joints
		- ![plot](https://user-images.githubusercontent.com/69032315/149354919-34fb65f8-821c-403a-b682-be6b9f7aa94a.png) 의 데이터셋은 다음과 같이 정해진다  ![plot](https://user-images.githubusercontent.com/69032315/149354952-a0d0b1a4-7080-4d9f-9044-b62450f556ea.png)
			-  ![plot](https://user-images.githubusercontent.com/69032315/149354982-d97a7767-3577-454e-97ab-71898d263c3f.png) = 각 픽셀  ![plot](https://user-images.githubusercontent.com/69032315/149355006-aa7896b3-da9d-4323-9ce9-3179d9fb83ec.png)의 integer index of joint, 
		-  ![plot](https://user-images.githubusercontent.com/69032315/149355109-fcac5a2e-7057-4c64-bad1-168e3ec13ac4.png) = 가려진 관절에 대한 investigation -> ground truth를 가리고 있는 pixel의 feature를 학습한다
		-  ![plot](https://user-images.githubusercontent.com/69032315/149355125-35e4dcd9-c9c4-4829-b8c4-b3b82f20c0f9.png)
		- ![plot](https://user-images.githubusercontent.com/69032315/149355139-563e90b4-7bb0-4956-a851-96929de4686a.png) = ground truth joint where V is 0
		- 앞의 데이터셋과 같이 ![plot](https://user-images.githubusercontent.com/69032315/149355152-8149b9b0-2aec-4a83-9e2f-d3a0d0c77370.png) 로 정의 된다

- Training Decision Trees
	- 이 논문에서 등장하는 모든 Decision tree들은 다음과 같이 정의 되는 depth difference를 사용한다 
	-  ![plot](https://user-images.githubusercontent.com/69032315/149355208-71ce0972-9884-4b29-acf3-63faf0f7d66f.png)
	- p = pixel on foreground, ![plot](https://user-images.githubusercontent.com/69032315/149355224-77be2b20-79ec-49da-b6f2-79125e8a8d0a.png)  = set of two randomly sampled 2D offsets-> I(p)로 compensate된다, 
	- ![plot](https://user-images.githubusercontent.com/69032315/149355238-5d57c679-09f5-4d28-b56f-cf402b775f30.png) 의 threshold인 ![plot](https://user-images.githubusercontent.com/69032315/149355260-4d380227-6954-4f38-8e0c-89b4679b0a4b.png) 보다 큰 값의 pixel만 남겨두고 ![plot](https://user-images.githubusercontent.com/69032315/149355288-d1b84c6d-5e10-4bd5-aa70-414782e99ade.png) 로 sampling되고 낮으면 ![plot](https://user-images.githubusercontent.com/69032315/149355311-40f04f60-2bf8-43cc-8cba-6879ec2c6712.png) 로 sampling된다 

	- 각 노드들은 파라메터 를 통해 목적함수 E를 최적화 시키는 것으로 학습을 한다 
	- E의 결과는 목적함수 ![plot](https://user-images.githubusercontent.com/69032315/149355322-e6d9d5db-7cb1-4eb3-8690-4cb6749c8fdb.png) 의 값들을 합하는 방식으로 산출된다
	-  ![plot](https://user-images.githubusercontent.com/69032315/149356088-3ff85529-62dc-44cd-af74-3b16fedfddbf.png)
	- 목적함수 E는 decision tree의 유형에 따라 바뀔 수 있고 ![plot](https://user-images.githubusercontent.com/69032315/149355356-ea053dcb-8360-4bfa-901d-94790db05ec8.png)  = MSE를 목적함수로 가진다, 이상치의 영향을 줄이기 위해서 threshold ![plot](https://user-images.githubusercontent.com/69032315/149355378-7eab9ba2-70cd-4b47-9e0a-3ea3a434273b.png)(length of the offset)을 설정해주고 ![plot](https://user-images.githubusercontent.com/69032315/149355405-b11b0ac2-f4e8-4784-91d8-dac804621d0a.png) 보다 긴 offsets은 제외시켜 주었다( ![plot](https://user-images.githubusercontent.com/69032315/149355425-26121532-6388-47eb-96d6-b28bd85f2ecd.png)는 경험적으로 500mm로 설정해주었다)

	- ![plot](https://user-images.githubusercontent.com/69032315/149356210-68c34a8f-b336-47c0-bbc7-2826ef4e466d.png)
	- ![plot](https://user-images.githubusercontent.com/69032315/149355463-6cb50344-cfac-4428-8dee-93c9bf098e2c.png)
	- ![plot](https://user-images.githubusercontent.com/69032315/149355486-6a6b5615-8e4a-48bb-aecf-134ca5696721.png) = 데이터셋 S에서 joint j의 정규화된 histogram 

- Procedure of the Algorithm
	- 1. ![plot](https://user-images.githubusercontent.com/69032315/149355552-9de74d10-bacd-4394-adb0-7bbe7b79ddba.png) 를 실행하여 vote를 casting한다 
	- 2. 픽셀의 가시성에 따라 ![plot](https://user-images.githubusercontent.com/69032315/149355568-24a1cb01-c474-4b09-bfa5-9cbb4aaf64a0.png)  를 적용시켜주고 vote를 casting한다
	- 3. 3개의 값들을 weighted하여 각 관절에 대한 center를 joint로 예측한다

- Weight models
	- 각 모델들의 voting에 대한 weight를 구해준다
	-  ![plot](https://user-images.githubusercontent.com/69032315/149355588-ad819097-70de-431c-818d-c3d9350e49fd.png)
		-  ![plot](https://user-images.githubusercontent.com/69032315/149355603-46aabd69-8721-49a6-8355-d981fe31efe8.png)= confidence values from  ![plot](https://user-images.githubusercontent.com/69032315/149355623-7d9a8580-654d-4d2d-9103-9ecb92e8839d.png)
		-  ![plot](https://user-images.githubusercontent.com/69032315/149355637-8d36b5ee-ef65-4545-8c4b-00926d648579.png)(가려진 부위)에서 많은 수의 보이지 않는 부분에 대한 vote는 제거되었다 
	- 위의 식을 가려진 부위에 대한 예측을 하면 많은 수의 joint에 대한 값들이 날라가게 된다(가려진 부위의 값이 0이되면 최종 값이 0이 되기 때문에)
	-  ![plot](https://user-images.githubusercontent.com/69032315/149355652-72393c26-f172-453a-8d7d-565c959383d7.png)
	- (11)번 식에서는 alpha 값을 0.99로 설정을 해주어서  ![plot](https://user-images.githubusercontent.com/69032315/149355670-d76b4784-e159-4a8f-89fa-abcd2957e4bf.png)값이 0이어도 weight가 0이 되지 않는다
	
	- ※ ![plot](https://user-images.githubusercontent.com/69032315/149355689-4343b52f-7506-49eb-94a1-c960451b2729.png) 에 각각  ![plot](https://user-images.githubusercontent.com/69032315/149355699-8c4fa0bd-52ba-43eb-b6d7-4af43eca183b.png), ![plot](https://user-images.githubusercontent.com/69032315/149355714-3b7822b2-90a8-494c-a36c-d1219d51b060.png)를 사용해 주었다 












