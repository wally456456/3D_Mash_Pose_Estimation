# 논문 제목 : Encoder-decoder with Multi-level Attention for 3D Human Shape and Pose Estimation(ICCV 2021)
### 저자 : Ziniu Wan1* Zhengjia Li2* Maoqing Tian3 1 Carnegie Mellon University 3 SenseTime Research Jianbo Liu4 Shuai Yi3 2 Tongji University 4 Chinese University of Hong Kong Hongsheng Li4

모르는 단어 : monocular(단안의), cluttered background(혼란 배경), trajectory(탄도,궤적)

전문 단어 :  MAED(Attention Encoder-Decoder Network), STE(Spatial-Temporal Encoder), KTD(Kinematic Topology Decoder), MSA(Multi-Head Self-Attention)

# Introduction
- 기존 방법들은 cluttered background, occlusion, extreme pose등의 상황에서 좋은 성능을 발휘하지 못하는 문제점들이 있었다 
- 위의 문제점들을 해결하기 위해서 3가지의 intrinsic 관계를 관절의 관점에서 모델링 된다
	- 1. Spatial relation : 혼란한 배경을 가지고 있는 사진이나 비디오일수록 모든 신체부위 예측을 하기 힘들다 -> 몸의 상대적인 신체부위의 위치적 관계를 가지고 추론을 통해 더 좋은 예측을 할 수 잇다 
	- 2. Temporal relation : 사진마다 각각이 가지고 있는 특성을 고려(가려져 있거나 하는 부분에 대한 추론을 잘하게 하는 것)
	- 3. Human joint relation : SMPL 모델의 경우 `Kinematic Tree`의 형태를 가진다. 따라서 각 관절들의 관계를 잘 파악하는 것이 중요하다
		- `Kinematic Tree` : 부모 관절이 움직일 경우 자녀 관절이 따라 움직이는 형태를 가진 메커니즘 
## 요약
- 저자들은 MSA-S(Multi-Head Self-Attention-Spatial relation)과 MSA-T(Multi-Head Self-Attention-Temporal relation)을 활용하여 공간과 temporal 한 정보를 encoding 한다 
- 저자들은 KTD를 사용하여 인간의 관절에 대한 계층적 의존성을 활용하고 human joint level attention을 얻어낸다

![plot](https://user-images.githubusercontent.com/69032315/147862903-585ee171-d667-4596-a5bf-e9273d362d7f.png)
	- 위의 그림은 KTD의 Top-down regression progress 를 설명한다. 즉 parent joint의 정보를 활용 하여 children joint까지의 관절에 대한 예측을 해준다 
  
  
  # Method
- SMPL
	- 인간의 Mesh를 만들어주는 모델
	- Shape parameter ![plot](https://user-images.githubusercontent.com/69032315/147862910-74d2add9-c23b-4c85-a48d-b6ed235d9d0e.png)
 과 Pose parameter  를 input으로 받고![plot](https://user-images.githubusercontent.com/69032315/147862911-3a2687f8-159d-46f5-add4-6ef3c333cbd8.png)
 ![plot](https://user-images.githubusercontent.com/69032315/147862914-675a3266-6c41-42ce-a59a-1f047043a3b7.png) 를 output으로 낸다
 
- Framework Overview

  ![plot](https://user-images.githubusercontent.com/69032315/147862915-46739f06-85d7-4ca5-855f-c83ac154008e.png)

  - T(Length of Video clip)를 input으로 받고 CNN backbone을 활용하여 기본적인 특징을 각 프레임마다 뽑아준다
  - `CNN층에 대한 설명` :CNN 층들 끝에 있는 global pooling layer를 제거해 줌으로써 각 T마다의 feature map의 크기를 (h x w x d)로 맞춰준다 -> 1D Sequence로 바꾸어 준다( h x w x d -> hw x d)(VIT에 넣어 주기 위해서 path로 만들어주는 작업을 해준 것) -> 따라서 CNN 의 output matrix의 사이즈는 (T x N x d where N = hw +1)이다

  - 위의 과정을 거친 후 spatial-temporal modeling을 거친 후 STE에 투입이 되어 encoded vector 를 만들어내고 이것을 KTD에 투입 시켜 주어서 SMPL에서 필요한 parameter인 ![plot](https://user-images.githubusercontent.com/69032315/147862923-71fcc22e-bcbe-446f-a3cc-8f8d7d7e33ca.png)
 를 예측하게 한다

	- Loss term에 대한 설명 
		-  ![image](https://user-images.githubusercontent.com/69032315/147862927-435dc2eb-4fb4-4e84-99dc-6f3862467610.png)

		-  ![image](https://user-images.githubusercontent.com/69032315/147862928-c83caf3a-d411-4e98-a800-18e4a5f82edf.png) -> projection from 3D to 2D

			- L2D와 L3D는 key point loss, Lsmpl은 SMPL parameter loss, Lnorm은 L2-Normalization loss를 의미한다
			- ![plot](https://user-images.githubusercontent.com/69032315/147862941-197b218e-d611-4b88-861f-788de574835b.png)
 는 각 값의 ground-truth값을 의미한다  
  
  ![plot](https://user-images.githubusercontent.com/69032315/147862943-051997db-e235-473b-83ea-a53de64bb2f9.png)

- Spatial-Temporal Encoder
	- global pooling operation은 하나의 frame 안의 공간적 정보의 손실을 피할 수 없다 -> 디테일한 인간의 자세를 추측하기 힘들게 한다
	- 저자들의 모델은 아래와 같이 3가지의 형태를 띈다
 ![plot](https://user-images.githubusercontent.com/69032315/147862947-0b70d9ad-0ed0-4b19-983b-fad3af86b9bf.png)

	- MSA-S : Key spatial information(e.g. joints, limbs)을 찾으려고 하는 것 -> 각 attention의 head 마다 (T x N x N)의 heatmap 을 결과로 만들어낸다(연산은 scaled dot-multiplication) -> 프레임 간의 interaction은 고려하지 못한다(패치마다의 결과이지 패치 간의 관계를 파악하지 못한다)(T=시간 프레임, N = h x w)
	- MSA-T : MSA-S와 대부분의 과정이 같다 but input의 크기를 (T x N x d)에서 (N x T x d)로 바꾼다, 각 attention의 head 마다 (N x T x T)의 heatmap을 output으로 도출한다(heatmap의 의미는 각 frame마다 현재의 frame말고 전체적 시간적을(temporal) 얼마나 고려할 것인가)
	- MSA-C : patch sequence와 frame sequence를 합쳐서 flatten 시켜준다 T x N x d -> TN x d -> 이렇게 하면 heatmap의 size 가 (TN X TN)이 되는데 각 패치가 다른 패치와 상호작용을 할 수 있게 한다
![plot](https://user-images.githubusercontent.com/69032315/147862950-2e53f294-1ae5-4d0d-b879-3a3ef8830a49.png)
	- STE Blocks : Fig 3 에서의 구성 처럼 Block을 구성해준다 
		- Coupling block : spatial 과 temporal한 정보를 모두 사용하기 위한 block, `유의점` 이게 dot-multiplication연산을 활용하기 때문에 sequence의 길이가 커지면 복잡도가 4배가 커진다 
			- 구성 : MSA-C -> MLP 
		- Parallel block : MSA-T 모듈과 MSA_S 모듈을 평행하게 붙이는 것 -> element wise addition을 사용해서 두개의 값을 두함
		- Series block : 두 모듈을 직렬 방식으로 붙이는 것
    
![plot](https://user-images.githubusercontent.com/69032315/147862953-238f8892-652e-4409-8372-2548461db6b0.png)

- Spatial-Temporal Positional Encoding : 패치의 공간적, 시간적 정보를 고려한 배치를 위해서 두개의 Trainable한 positional encoding을 사용한다
	- Spatial positional encoding :  ![plot](https://user-images.githubusercontent.com/69032315/147862957-ab7c7cf4-f05b-4eae-9da8-a62238f322d8.png)

	- Temporal positional encoding :  ![plot](https://user-images.githubusercontent.com/69032315/147862960-f22e64ed-ed74-4c02-89f3-1dc3fba3c325.png)

	![plot](https://user-images.githubusercontent.com/69032315/147862961-2c7ea6c8-9961-4216-93ba-36197714d653.png)

	- Kinematic Topology Decoder(KTD) : joint의 level에서 model에 attention을 주기 위한 method
	- ![plot](https://user-images.githubusercontent.com/69032315/147862966-64e8b1bf-40ec-4520-a22e-f61c9ff5af68.png)  : joint k의 world transformation = kinematic tree에서 부모 joint들의 transformation matrices의 누적 합계
	
 ![plot](https://user-images.githubusercontent.com/69032315/147862968-20f33fd8-6b0b-4643-8a3f-b65400de8245.png)

-  ![plot](https://user-images.githubusercontent.com/69032315/147862970-dcd687fe-4329-4e68-b3fc-0b76d0bf2a75.png)

	-  ![plot](https://user-images.githubusercontent.com/69032315/147862973-45e59d12-a8c0-4977-aa27-b2844f0b3391.png) = joint k의  rotation matrix, ![plot](https://user-images.githubusercontent.com/69032315/147862976-76b4abc7-275d-439c-b0d0-7b053d65c1eb.png)
  = joint k의 translation vector
			- A(k) = joint k 의 부모 joint의 ordered set(e.g. A(5) = {0,2})
	- ※ 최근의 iterative feedback regressor는 parent joint에 더 많은 attention을 주지 않는다 -> 최상의 결과를 얻지 못한다 -> 저자들의 KTD는 이것을 해결할 수 있다

		- 1. shape와 cam parameter를 다음과 같이 분해한다
 ![plot](https://user-images.githubusercontent.com/69032315/147862981-d5b26267-dca4-403f-96d3-48345db1991a.png)

-  ![plot](https://user-images.githubusercontent.com/69032315/147862986-fc56fe9d-bfed-4389-a66f-f4dc44fcb8ea.png)
 
 <- 이것들은 STE에서 추출된 image feature 
		- 2. Kinematic tree의 계층적 구조에 따라 각 joint마다의 pose parameter를 생성한다(e.g. Figure 4의 {0,2,5} 관절들을 예로 들면, 첫번째로는 ![plot](https://user-images.githubusercontent.com/69032315/147862991-639cd970-afe9-4a9a-87f9-1163e0e2f302.png)
 를 활용해서 root(global body orientation)의 pose parameter를 예측한다 -> 두번째로는 6D rotation representation을 사용한다 -> 세번째로는 2번 관절의 image feature ![plot](https://user-images.githubusercontent.com/69032315/147862994-84d6859a-7afb-416e-9582-b126d9373aa3.png)
 를 Regressor ![plot](https://user-images.githubusercontent.com/69032315/147862997-bfa27a6c-b2f4-41f9-ac32-e0e5b81bdc33.png)
 로 투입 시켜주고 의 ![plot](https://user-images.githubusercontent.com/69032315/147863003-ba845d3b-3879-4334-a0ae-b8d2f20f546e.png)
 pose parameter를 얻는다) -> 모든 joint가 완료될 때 까지 반복
	- KTD를 통해서 저자들은 관절마다의 dependence를 정의한다 -> 기존의 방식은 parent자신의 error만을 고려한다 -> KTD의 경우에는 parent의 error가 children의 관절까지 영향을 준다 -> parent의 관절에 더 많은 attention을 주어 학습을 하게 한다


  
  
  
