# 논문명 : PARE Part Attention Regressor for 3D Human Body Estimation
### 저자명 : Muhammed Kocabas, Chun-Hao P. Huang, Otmar Hilliges, Michael J. Black

### 전문 용어 : HPS = Human pose and shape, part segmentation(물체의 일부분만 가지고 pixel 단위의 semantic annotation을 만드는 것)

# Abstract
- 기존의 3D pose, shape estimation은 가려진 사진에 대한 예측이 정확하지 않았다
- 저자들의 Approach는 각 individual의 보이는 신체정보를 활용하여 가려진 주변의 신체부위를 예측한다

# Introduction

![plot](https://user-images.githubusercontent.com/69032315/147872397-3eb83994-be5c-44fb-965b-184333605f8f.png)

- Figure 1에서 히트맵이 보여주는 것은 가려진(occluding patch) 부위에 대한 joint error이다
	- 즉 PARE는 가려진 부위에 대한 예측을 잘 수행하였고 반대로 SPIN은 수행하지 못하였다
- 이러한 점을 고치기 위해 PARE에서는 part-guided attention mechanism을 사용하였다
- PARE에서 두가지 task
	- 1.end-to-end 방식으로 3D body parameter를 regress 하는 것
	- 2. 보조적 업무 : 각 신체부위마다 attention weight를 학습하는 것
- Body-part-driven attention : 학습 초반에 attention branch를 part segmentation labels를 포함하여 학습을 시켜주고 후반에는 이를 제외시킨 다음에 학습을 진행한다
	- part segmentation = 물체의 일부분만 가지고 pixel 단위의 semantic annotation을 만드는 것)

- 저자들의 생각은 occlusion에 강건한 모델을 만들기 위해서는 network가 보이는 신체부위의 pixel-aligned(픽셀로 된) feature를 사용해야 된다고 생각한다
- attention mask를 part segmentation에 supervise 한 다음 pose에 대한 supervision만 수행하도록 한다 -> 모델이 신체와 주변에 대한 정보까지 고려하게 하기 위해서 -> 비지도 학습의 방식으로 모델이 찾는 영역에 대한 정보를 함께 고려하게 한다

# Occlusion Sensitivity Analysis
- I = input image region 
- 저자들은 gray occlusion patch를 sliding하면서 모델들의 가려진 부위에 대한 예측의 오차(유클리디안 거리)를 heatmap으로 표현해 준다 
![plot](https://user-images.githubusercontent.com/69032315/147872404-891d9fa5-22eb-4c87-a9ab-2f84a840afb1.png)

- Figure3에서는 각 부위를 가렸을 때 얼마나 오차가 심한지를 Mesh의 형태로 보여준다
- 이 분석을 SPIN 모델에 적용하였을 때 얻은 인사이트 4가지
	- 1.오차는 배경을 가렸을 때 보다 신체를 가렸을 때 더 올라간다
	- 2.원본 사진에서 Joints가 보였던 것이 안 보여지게 되면 더 큰 error를 발생하게 한다
	- 3.원래 보이지 않았던 부분을 가리면 모델이 다른 부위의 정보를 활용하여 잘 예측하게 된다
	- 4.occlusion의 영향력은 가까운 부위에서만 발생하는 것이 아니라 먼 부위에까지 영향을 줄 수 있다


# Method
![plot](https://user-images.githubusercontent.com/69032315/147872421-79691203-9123-4d8e-89d2-2b27d0e5707d.png)

- 기존 방식들은 spatial information을 얻을 때 global average pooling을 사용하기 때문에 많은 정보의 손실이 불가피하다
- PARE는 기존의 방식들과 다르게 pixel-aligned representation(Feature volume)을 사용해서 위의 문제점들을 없앤다
- Attention weight와 end-to-end trainable features for 3D pose가 다른 task이기 때문에 2가지의 feature volume을 사용해준다
	- Attention weights를 train하는 방법 : 2D part branch를 사용하는 것
	- end-to-end trainable features for 3D pose(SMPL parameter regression)를 train 하는 방법 :3D body branch를 활용하는 방법

- Preliminaries : Body Model(SMPL)
	- SMPL은 pose parameter ![plot](https://user-images.githubusercontent.com/69032315/147872426-ed2cd62d-8969-4c63-8de9-e2b0e6566623.png) 과 shape parameter  를 활용해서 3D mesh  ![plot](https://user-images.githubusercontent.com/69032315/147872430-7d90ed2a-d21c-4503-be02-c6fb4788421b.png)를 만들어내는 모델이다 
	- 3D joint location은 ![plot](https://user-images.githubusercontent.com/69032315/147872437-06424c35-71c0-45ab-bfe2-b0c1c59c52e8.png)
 는 사전학습된 linear regressor W를 통해서 예측되어 진다

- Model Architecture and Losses
	- 모델의 학습과정
		- CNN backbone을 사용하여 volumetric features를 추출한다(No global average pooling!!!)
		- 2개의 feature extraction branch를 이용하여 volumetric image feature를 2개 뽑는다
			- 2D part branch =  ![plot](https://user-images.githubusercontent.com/69032315/147872440-55c85063-7962-4a46-bd76-2eba421d312f.png)<- 신체부위 J에 대한 part attention(back ground mask로는 1, H와 W는 feature volume의 height & width이다 -> 즉 각 픽셀마다의 신체부위 j(소문자임!!!)로부터 얼마나 연관되어 있는 지를 표현
			- 3D body parameter estimation =  ![plot](https://user-images.githubusercontent.com/69032315/147872443-dc181a9e-c941-438c-b7e2-d97458304748.png)	
			- ![plot](https://user-images.githubusercontent.com/69032315/147872450-d09204ef-8722-4149-bab9-8882db173f34.png) 는 각각 P의 j번째 관절과 F의 c번째 채널을 의미, ![plot](https://user-images.githubusercontent.com/69032315/147872453-39f14a99-3b8e-47e0-ba6a-994d1d32d334.png) 는 final feature tensor를 의미
		-  Fc의 각 element들은 F’에 Pj에서 얻은 값에서 spatial softmax normalization을 기반으로 비율적으로 기여한다 (Pj에서 높은 값을 얻었으면 해당 Fc의 값이 더 크게 반영이 된다!!!)
			- σ = spatial softmax normalization
			- (j,c)의 위치에 있는 F’ element는 다음과 같이 계산된다


  ![plot](https://user-images.githubusercontent.com/69032315/147872458-27929e0c-8b72-466d-aa40-89553ae34011.png)

  - ![plot](https://user-images.githubusercontent.com/69032315/147872464-7b7893ef-ec76-477e-9db6-40ea22a47692.png) = Hadamard product
	  - Hadamard product

  ![plot](https://user-images.githubusercontent.com/69032315/147872468-0aef8039-8c34-430b-aa8e-e2ec72a7bb00.png)

	- 보이는 신체부위 : 2D part branch P를 ground-truth segmentation label을 사용하여 supervise해준다 <- 보이는 신체부위의 attention map이 알맞은 부위로 수렴하게 한다
	- 보이지 않는 신체부위 : 이 경우에는 ground-truth segmentation label이 존재하지 않기 때문에 attention weight가 0이 되게 된다 따라서 2D part branch를 모델의 초기 단계에서만 사용한다 -> 초기 단계 이후로는 unsupervise의 방식을 채택한다(이 방식은 pose를 추측하는데 필요한 joints의 위치가 아닌 다른 영역의 정보를 얻어 모델이 좋은 성능을 내게 한다)
	
	- Full feature tensor  ![plot](https://user-images.githubusercontent.com/69032315/147872478-4f12b1e0-884c-45f5-bd93-47702ccd1e40.png)가 body shape β와 weak-perspective camera model를 예측하게 한다
		- weak-perspective camera model(with scale and translation parameters  ![plot](https://user-images.githubusercontent.com/69032315/147872494-b877e2d3-591b-403c-9a0d-a6d5b9abad49.png)
	- 동시에  ![plot](https://user-images.githubusercontent.com/69032315/147872498-42908f35-8b45-4824-820c-b389c1a5aa06.png)(F’의 j번째 joint의 행)은 MLP에 보내져 ![plot](https://user-images.githubusercontent.com/69032315/147872512-05645449-66f3-49cd-aa10-17f721cdc6ac.png) (각 부위의 rotation of each part, 6D vector)를 예측하게 한다 

	- Overall Loss
  ![plot](https://user-images.githubusercontent.com/69032315/147872514-9dd23c9d-8035-4bdf-af79-689de8e12c79.png)

	-  ![plot](https://user-images.githubusercontent.com/69032315/147872520-30ff2cac-4db0-49ef-b209-2e7b41f12d9e.png) = ground truth of variable x(hat은 ground truth값이라는 뜻)
	- 2D keypoint loss를 계산하기 위해서 ![plot](https://user-images.githubusercontent.com/69032315/147872527-5e28ddc6-40e1-4544-b3fe-a786cd7990b9.png) (SMPL 3D joint locations)과 weak-perspective camera를 활용해서 구해야 한다 -> 3D joint points를 2D joints로 projection하는 방법
		-  ![plot](https://user-images.githubusercontent.com/69032315/147872534-8b8ef9d6-b8f0-4f1f-8acf-01f381f02bc6.png)
			-  ![plot](https://user-images.githubusercontent.com/69032315/147872541-1afb3c3d-825f-4bf8-af18-cd5863e27114.png) = camera rotation matrix, Π = orthographic projection, λ = scalar coefficient to balance the loss term, (s,t) = camera parameter
	-  ![plot](https://user-images.githubusercontent.com/69032315/147872545-9ced8c0c-d4a4-4798-be69-357790e9fadf.png) = (h,w) 위치에 있는 P의 값
	- ![plot](https://user-images.githubusercontent.com/69032315/147872548-713c14f5-1e83-4321-a84c-4fc6ad4996ab.png)  = (h,w)위치에 있는 P의 ground-truth 값


- Implementation Details
	- 앞에서 언급했듯이 loss term Lp은 초기 단계에서만 사용한다 나중에는 λ값을 0으로 만들어준다
	

















