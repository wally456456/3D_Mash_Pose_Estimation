# 논문명 : Collaborative Regression of Expressive Bodies using Moderation
### 저자명 : Yao Feng, Vasileios Choutas, Timo Bolkart, Dimitrios Tzionas, Michael J. Black
#### 3D Body Mesh SOTA 논문입니다

# Abstract

- Body에 대한 예측은 뛰어나지만 얼굴에 대한 예측은 성능이 좋지 않다
- PIXIE의 두가지 특징
	- 1. 얼굴과 몸에 대한 예측이 따로 하는 경향이 있지만 PIXIE는 두개를 함께 고려한다
	- 2. 성별에 대한 고려를 한다


# Introduction

- 각 신체부위에 confidence를 부여해서 각 feature에 가중치를 준다
- 모델이 학습과정에서 implicitly 하게 성별을 예측하게 하여 inference를 할 때는 gender label이 없어도 gender의 특성을 고려한 body shape를 추론할 수 있다
- face expert에 facial albedo와 3D facial-surface displacement를 추측하는 branches를 추가해주었다


# Method
- SMPL-X를 사용해서 하나의 RGB image로부터 body mesh를 만들어낸다
- Body, Face/head, hand regression 모델 3개를 사용한다
- 3가지 새로운 점
	- 1. 각 모델의 feature를 결합하고 confidence 스코어로 weight시켜주어 모호한 부분에 대한 추론을 잘하게 해주는 moderator를 만들었다
	- 2. 성별을 추론할 수 있게 하는 shape loss를 사용한다
	- 3. Face expert(Face의 parameter를 예측하는 모델)에서 얻은 albedo에 surface detail branch를 추가해 주었다

- Expressive 3D Body Model
	- SMPL-X를 사용해 주었다(body shape + facial expression + finger)
		- SMPL-X 의 함수 : ![plot](https://user-images.githubusercontent.com/69032315/148206422-b41a2af9-0d07-4f39-969f-36b6cdc6ed77.png)
 , β = shape, θ = pose,  ![plot](https://user-images.githubusercontent.com/69032315/148206428-6d682430-6d57-4557-9c5e-74479a88e5da.png)
= expression
		-  ![plot](https://user-images.githubusercontent.com/69032315/148206438-0c1c010e-1bb2-452a-8ea7-065a99dcbd79.png)
 = coefficients of a linear shape space <- body, 얼굴, 손의 shape correlation을 의미하는 관절 shape공간
		- ![plot](https://user-images.githubusercontent.com/69032315/148206453-1b37fa58-fa1b-4011-9eb0-1e0f145a66af.png)
 = coefficient of a low-dimensional linear space(얼굴의 expression)
		- θ = body, jaw(턱), hand pose의 vector -> 턱을 제외한 각 관절의 rotation은 6D vector로 인코딩 되어있다, 턱은 오일러 각을 이용해서 3차원의 vector로 인코딩
		- posed joint를 ![plot](https://user-images.githubusercontent.com/69032315/148206467-ed2aab24-3d04-4b95-9c36-0ecc15fca88d.png)
 로 표현한다 , J = 55
	- Camera : weak-perspective camera 모델을 스케일 vector인 ![image](https://user-images.githubusercontent.com/69032315/148206480-bcc9c348-25ff-4377-8e61-a6a2f04c7439.png)
 과 translation ![plot](https://user-images.githubusercontent.com/69032315/148206494-7051800c-21dc-4274-85d4-9815f985181c.png)
 를 사용해서 SMPL-X 를 이미지로부터 재건한다
	- X(joints), M(model vertices)는 ![plot](https://user-images.githubusercontent.com/69032315/148206512-243f65b1-040a-4fb1-9b65-89c216c17938.png)
 로부터 투영된 값이다

- PIXIE Architecture
  - ![plot](https://user-images.githubusercontent.com/69032315/148206543-04553362-02ff-4fb4-9aec-811fb29ec589.png)

	- Input images : I(image)에 bounding box를 사용해서 body를 뽑아내고 attention mechanism을 사용해서 face/head부분![plot](https://user-images.githubusercontent.com/69032315/148206581-b684b325-921d-44c1-b6eb-0404e0d2d1c6.png)과 손 부분인 ![plot](https://user-images.githubusercontent.com/69032315/148206598-c394e7e9-b8ca-440b-858a-d1fea027b9e3.png)
를 추출한다
	- Feature encoding : 각 추출한 이미지들을 인코더 ![plot](https://user-images.githubusercontent.com/69032315/148206658-24586821-8432-467e-afdd-c90b584712f2.png)
에 넣어주어 feature ![plot](https://user-images.githubusercontent.com/69032315/148206668-63010d33-fc18-4acd-bb24-fc3b4c11f085.png)
 를 추출해준다
		- face/head, hand 인코더로는 ResNet-50 을 사용하여 ![plot](https://user-images.githubusercontent.com/69032315/148206677-f7f7bb93-de92-4f0d-8dc8-421a04f0a41e.png)
 의 백터를 만든다
		- Body expert 인코더로는 HRNet을 사용해주어 ![plot](https://user-images.githubusercontent.com/69032315/148206687-39399d23-65d8-4b22-930c-ae8c1977e4d2.png)
 의 백터를 만든다

- Feature fusion(moderator) 
	- {body,head}, {body,hand} 쌍의 feature map들을 중재해주는 moderator ![plot](https://user-images.githubusercontent.com/69032315/148206738-e933dea1-13e5-4534-98fc-9b4f15769e33.png)
 를 사용하여 결합된 feature 인 ![plot](https://user-images.githubusercontent.com/69032315/148206751-19738d7c-ece2-4eac-bba5-f61e33f733a6.png)
 를 만들어낸다 -> 만들어진 결합된 feature를 ![plot](https://user-images.githubusercontent.com/69032315/148206765-16d80c6a-5804-409e-a6d6-8970aeff6c7b.png)
 (face/head, hand regressor)에 넣어준다(Moderator는 MLP로 구성 되어있고  ![plot](https://user-images.githubusercontent.com/69032315/148206775-5195dbee-c6db-4c41-ae6c-1990292da5bd.png)
(body)와 ![plot](https://user-images.githubusercontent.com/69032315/148215606-badae6c1-3ba2-4ab3-9c10-7d835a970e15.png) (face/head, hand)의 가중치들을 weighted sum하게 해준다)

- ![plot](https://user-images.githubusercontent.com/69032315/148215618-fe5e4f63-6e49-4c62-8b50-6edca85043c4.png)

	-  ![plot](https://user-images.githubusercontent.com/69032315/148215628-b540aeec-839a-42d9-bacc-c6ce2351038a.png)
 = part moderator,  ![plot](https://user-images.githubusercontent.com/69032315/148215660-fa79463d-a3fc-4f92-864f-0eca2bd8d8ef.png)
 = expert’s confidence, ![plot](https://user-images.githubusercontent.com/69032315/148215670-1504786f-97cc-410a-9ec3-7c9bd17139c1.png)
 = body feature,  ![plot](https://user-images.githubusercontent.com/69032315/148215678-f1377582-4611-467e-86a0-3175a9e7c64f.png)
= body 인코더와 moderator 사이의 linear 층, ,t = learned temperature weight(갱신되지 않음), 
	- Parameter regression 
	- 1. ![plot](https://user-images.githubusercontent.com/69032315/148215693-500ba516-ab31-41e5-9bf2-fb45a5eb242a.png)
 (regressors)를 ![plot](https://user-images.githubusercontent.com/69032315/148215727-b6e75f9c-4a2c-4252-bb9b-79c55e64f775.png)
 (moderator 를 거치지 않은)값들 만을 사용하는 경우
		-  ![plot](https://user-images.githubusercontent.com/69032315/148215741-23665b25-3dfb-47bc-becb-2684dd28d804.png)
는 카메라 parameter ![plot](https://user-images.githubusercontent.com/69032315/148215753-99527ad1-4218-4eb8-8ef7-277f5b96bb13.png)
 과 머리와 손목을 제외한 body rotation과 포즈 를 infer한다 
		- ![plot](https://user-images.githubusercontent.com/69032315/148215757-66e601dd-7b78-4d6f-bc70-f123195a48d6.png)
 는 카메라 parameter ![plot](https://user-images.githubusercontent.com/69032315/148215768-ef94a45c-16e9-40e2-9b61-057da97e5a06.png)
 , face albedo ![plot](https://user-images.githubusercontent.com/69032315/148215776-f5a8076e-f828-41d9-a6f8-4b63989c74ed.png)
 , lighting ![plot](https://user-images.githubusercontent.com/69032315/148215779-11cbecf3-bd00-4675-9799-7236bc485e62.png)
 를 infer한다 
		- ![plot](https://user-images.githubusercontent.com/69032315/148215784-10bca354-4396-40e8-937a-d43ad0be8e97.png)
 는 카메라 파라메터 ![plot](https://user-images.githubusercontent.com/69032315/148215795-22df827a-0de3-41ef-9bca-a317a6fd0ac2.png)
 를 추론한다
 	- 2.  ![plot](https://user-images.githubusercontent.com/69032315/148215808-bdc777d4-c2e8-4032-933e-62fad3d164cd.png)
, ![plot](https://user-images.githubusercontent.com/69032315/148215815-bde3b2fc-7ec1-4997-ad9e-3e6cb70ba10b.png)
를 결합된 feature  ![plot](https://user-images.githubusercontent.com/69032315/148215820-b4973fb6-83c6-44b4-9f30-d756a7d53688.png)
, ![plot](https://user-images.githubusercontent.com/69032315/148215826-c1d4c7cc-165d-4cce-881f-c85f2d4dc88c.png)
에 사용한다
		- ![plot](https://user-images.githubusercontent.com/69032315/148215862-48037986-f564-497a-8515-1717c753e265.png)
 는  (손목)와 ![plot](https://user-images.githubusercontent.com/69032315/148215869-23325a27-316c-4869-9924-b0888a0e5f80.png)
 (손가락)을 infer한다
		-  ![plot](https://user-images.githubusercontent.com/69032315/148215958-b85dd1bf-243c-4995-b1b4-d38d4fc7362b.png)
는 ![plot](https://user-images.githubusercontent.com/69032315/148215966-822351b4-28a7-44a4-aa5f-1e083088fc77.png)
 (expression), ![plot](https://user-images.githubusercontent.com/69032315/148215976-c21717f3-5ecc-4482-ba48-d63ac20fcc56.png)
 ,![plot](https://user-images.githubusercontent.com/69032315/148215996-9f2a9231-46ca-47a9-886d-12b4ec14bf83.png)
 를 추론한다 -> body shape ![plot](https://user-images.githubusercontent.com/69032315/148216003-c822d295-6fba-42be-b97a-6cf45d5cbceb.png)
 또한 infer해서 whole-body shape에 대해 기여를 하게 만든다
	- Detail capture : ![plot](https://user-images.githubusercontent.com/69032315/148216020-d6177507-83a4-43c2-b5b2-3e3b76f92378.png)
 (fine geometric details branch)를 사용해서 주어진 ![plot](https://user-images.githubusercontent.com/69032315/148216048-0edf3a3f-6b31-4296-8d13-2e420cd95b3d.png)
 (face image)를 FLAMES의 surface에 3D displacement시키게 한다 
		- UV map을 사용하여서 FLAMES의 displacement형식을 SMPL-X에 변환시켜주고 PIXIE의 추출된 head shape를 적용시켜준다 <- 얼굴에 대한 예측 값들은 full-body이미지에서 좋지 않은 성능을 보여주기 때문에 face/head expert의 confident 값이 높을 때만 사용하여 준다

- Training Losses
	- ![plot](https://user-images.githubusercontent.com/69032315/148216067-c7f8efe4-d68a-49f9-9f5f-5746170594fe.png)
	- ^ = ground truth

	- Body losses  : 2D re-projection과 3D joints, SMPL-X parameter loss를 구한다
  
	- ![plot](https://user-images.githubusercontent.com/69032315/148216099-95feb966-5e82-407c-9f96-0809053bcc02.png)

	- Hand losses : 3D hand pose와 shape의 차이
	- ![plot](https://user-images.githubusercontent.com/69032315/148216120-6a3d3c84-1762-4ecc-8031-515ed4edb16e.png)
 

	- Face losses : 3D face estimation 커뮤니티에서 흔히 사용되는 loss를 사용
 	- ![plot](https://user-images.githubusercontent.com/69032315/148216138-28c22f2c-29fd-4cda-938a-b5653de6081c.png)

 
	- landmark loss는 2D landmark  ![plot](https://user-images.githubusercontent.com/69032315/148216146-605afe3a-ba77-4c74-ae7e-3c556425e141.png)과 ![image](https://user-images.githubusercontent.com/69032315/148216157-a5052439-79b1-4a0c-be10-c4dee14a0485.png) 의 차이를 구하는 것

 	- 눈꺼풀 위/아래의 land mark들과 입술 위/아래의 landmark간의 차이

- ![plot](https://user-images.githubusercontent.com/69032315/148216200-6b597ddf-7cf2-49fb-b8c5-b53a750c7b76.png)


	-  ![plot](https://user-images.githubusercontent.com/69032315/148216263-fa1f607f-f2af-4248-9f3b-1949cd9c128d.png)
(face mesh)( albedo ![plot](https://user-images.githubusercontent.com/69032315/148216272-ab3b9e16-7170-4c5f-b8a7-d6d406ea8e8b.png)
, ligthing ![image](https://user-images.githubusercontent.com/69032315/148216277-1057596e-e37d-4e47-ab28-0de54797a2e7.png)
 로 구성된)를 rendering해주어서 이미지 ![plot](https://user-images.githubusercontent.com/69032315/148216289-713dca0b-0a78-4ca2-bb5b-384dc5a41c18.png)
 를 만들어주고 input face image  ![plot](https://user-images.githubusercontent.com/69032315/148216298-2a3b6529-1ab2-45b8-9488-346552d314f7.png)
와의 차이를 최소화하게 한다
 
- ![plot](https://user-images.githubusercontent.com/69032315/148216310-81e1f4de-0039-4d0b-abc2-445f4f517ba5.png)
		- S = binary face mask, ![plot](https://user-images.githubusercontent.com/69032315/148216349-6ad0e7d0-3392-4911-bac3-0b0d4db0ac2f.png)  = Hadamard product
		- Nirkin 의 segmentation network를 이용해서 S를 만들어낸다

	- Pre-trained face recognition network 를 사용해서  과  의 embedding을 만들어준다 -> 두 embedding의 cosine 유사도를 구해준다
 ![plot](https://user-images.githubusercontent.com/69032315/148216359-f09f36a4-8f4c-45e6-b21c-48edd271f294.png)
	- 추가적인 prior를 통해서 PIXIE가 더 plausible solution을 생성하게 돕는다
	- Gaussian 사전분포를 사용해서 expression parameter에 적용해준다
 ![plot](https://user-images.githubusercontent.com/69032315/148216391-4a99ebf2-1dea-42a5-8d00-17b66700e76f.png)
	- 턱(jaw)와 얼굴(face)에 soft regularization을 추가해준다
![plot](https://user-images.githubusercontent.com/69032315/148216409-15b50ef8-fb81-4316-9ce7-5a3af64c13b7.png)
	- prior들은 standard regularizer이고 경험적으로 얻어낸 값들이다
	- Gender : 각 성별의 shape parameter의 평균 와 공분산  을 구해준 다음에 
 
 ![plot](https://user-images.githubusercontent.com/69032315/148216476-01f0bd03-68ee-4f5f-9224-d8be1c9bc20a.png)
	- 성별을 알 수 없는 경우 모든 scans를 고려한 Gaussian prior를 사용한다
	- Feature update loss : transformed body feature ![image](https://user-images.githubusercontent.com/69032315/148216540-884e150d-d687-42bc-a8cd-450f2fd8b2d7.png)
 가  ![plot](https://user-images.githubusercontent.com/69032315/148216546-70827e06-5a2c-4665-8186-3e856edd3f5e.png)
와 같게 만들어주면서 모델을 안정화 할 수 있었다
![plot](https://user-images.githubusercontent.com/69032315/148216553-aee8b8b9-787e-4d5e-8853-ff58836bde27.png)

 





























