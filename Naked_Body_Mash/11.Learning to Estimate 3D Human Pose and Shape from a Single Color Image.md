# 논문명 : Learning to Estimate 3D Human Pose and Shape from a Single Color Image(CVPR 2018)
### 저자명 : Georgios Pavlakos,  Luyang  Zhu,  Xiaowei Zhou,  Kostas  Daniilidis


# Abstract
- 이 논문은 하나의 이미지를 가지고 full body 3D human pose task를 다룬다
- ConvNet을 활용하여 SMPL parameter를 얻는다
- SMPL parameter로 3D body mesh를 만든 후 ground-truth mesh와의 차이를 3D per-vertex loss를 사용해주어서 구해준다
- 마지막 `Renderer`는 3D mesh를 image에 투영시킨다 -> 추가 정재 작업을 용이하게 한다

![plot](https://user-images.githubusercontent.com/69032315/150369941-ffe42d36-e21b-4472-b8ed-327741141048.png)

# Introduction
- SMPL에 투입되는 파라메터들을 뽑아내는 Conv층을 사용한다 -> SMPL : 72개의 pose parameter, 10개의 shape parameter를 활용하여 6890개의 vertices를 만드는 통계적 모델
- Renderer를 활용하여 원본 이미지에 해당되는 실루엣과 key point들을 표현해 줄 수 있다


# Human body shape models
- SMPL을 활용하면 저차원의 파라메터를 활용하여 3D mesh를 표현해 줄 수 있다는 것이다
- SMPL = ![plot](https://user-images.githubusercontent.com/69032315/150369989-e01d2f6e-1e5b-4a6d-a7e0-5ca9e6c499fe.png) ,  `β` = shape parameter, `θ` = pose parameter, `Φ` = fixed parameters of models,  Output = ![plot](https://user-images.githubusercontent.com/69032315/150370023-2364e73d-c74b-42e7-b2b1-0a70dbe9c40d.png) (N = 6890 vertices,  ![plot](https://user-images.githubusercontent.com/69032315/150370032-ff719233-2970-415f-9ac4-e60e958208cc.png))

# Technical approach
- CNN based approach의 목적 2가지
	- 1. 이미지에서 keypoint를 추측하는 것
	- 2. 3D pose , shape를 2D 이미지에서 예측하는 것

- 4.1 Keypoints and silhouette prediction
	- 이전 연구들은 실루엣과 히트맵을 만드는 각자의 Conv Net을 활용하였지만 논문에서는 `Human2D` 모델 하나를 활용하여 Heatmap과 실루엣을 한번에 생성하게 한다
	- `Human 2D`모델은 Stacked Hourglass 의 구조를 사용한다
	- `Keypoint의 output`은 MSE Loss ![plot](https://user-images.githubusercontent.com/69032315/150370104-cf53a88a-aaa0-4ab0-93cf-bdec6ad42083.png)를 활용하고 heatmap의 형식으로 표현된다
	- `실루엣의 output`은 2개의 channel로 나뉘게 되고(Body, background), pixel wise로 표현되어 binary entropy loss ![plot](https://user-images.githubusercontent.com/69032315/150370131-302f2286-6930-40c2-8ea6-3cdcad6d060a.png)를 활용한다
	- 두개를 한번에 training 시키기 위해서 ![plot](https://user-images.githubusercontent.com/69032315/150370141-59ceae02-5468-4bd6-8c01-601323745d36.png)  같이 weighted sum을 활용한 Loss를 정의하게 된다

- 4.2 3D pose and shape prediction
	- 3D pose와 shape parameter를 2D keypoint와 실루엣을 활용하여 획득하는 과정
	- Mapping을 해주기 위해서 저자들은 2개의 network를 사용한다
		- 1. PosePrior : 2D keypoint의 위치를 confidence를 활용하여 input으로 투입 시켜주고 θ를 예측한다 
		- 2. Shape Prior : 실루엣을 input으로 사용하고 β를 예측한다

	- PosePrior Architecture
		- 2개의 Linear 층을 사용한다
	- Shape Prior Architecture
		- 5개의 3x3 CNN layer를 사용하고 각 층마다 Max pooling 밑 마지막에는 Linear 층을 사용해 주었다

	- 3D per-vertex loss
		- SMPL이 deterministic한 모델이기 때문에 이것을 하나의 layer층이라고 생각하고 backpropagation을 시켰다
		-  ![plot](https://user-images.githubusercontent.com/69032315/150370258-d1b07d63-4a32-439d-835f-f91702c49761.png)
		- ![plot](https://user-images.githubusercontent.com/69032315/150370283-10e4c919-f409-418b-af7f-ebfd89fa7a75.png) = predicted mesh vertices, ![plot](https://user-images.githubusercontent.com/69032315/150370306-d668660f-8a76-44ea-89c7-f07f8c501383.png)  = ground truth mesh vertices

- 4.3 Differentiable renderer
	- 3D mesh를 이미지에 mapping시켜주는 것
	- Renderer는 `OpenDR`를 활용하여 mesh나 3D joints를 이미지에 project시켜주고 backpropagation도 가능하게 한다
	- Projection Operantion(Π) 
		-   ![plot](https://user-images.githubusercontent.com/69032315/150370376-9d3d31b1-a394-43f9-8642-a3d47c739861.png)= 64x64 binary image,  ![plot](https://user-images.githubusercontent.com/69032315/150370408-728e79ed-676f-4e32-a415-d34555b03eca.png)(projected 2D joints)로 표현된다
		-  ![plot](https://user-images.githubusercontent.com/69032315/150370472-6d6add15-38c5-4045-b07a-8fc4fce86cdd.png)
		- µ = 10
		
















